{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mnist_dataset import ImageAndLabelSet\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w, img_h = 28, 28\n",
    "mnist_data_root = path.expanduser(\"~/data/mnist/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_input_batch = np.random.uniform(low=0, high=255, size=(32, 28, 28, 1))\n",
    "fake_input_gt = np.random.choice(10, size=(32))\n",
    "fake_input_gt_oh = np.zeros(shape=(32, 10), dtype=np.float32)\n",
    "fake_input_gt_oh[np.arange(32), fake_input_gt] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(input_tensor: tf.Tensor, in_filters: int, out_filters: int, stride=1) -> tf.Tensor:\n",
    "    # FIXME - may want to apply batch-norm here\n",
    "    t = tf.nn.relu(input_tensor)\n",
    "    \n",
    "    # Downsample for the skip connection\n",
    "    # FIXME - should this start close to an identity transformation?\n",
    "    kernel_ds_xavier_range = np.sqrt(6) / (np.sqrt(1*1*in_filters) + np.sqrt(out_filters))\n",
    "    kernel_ds = tf.Variable(tf.random.uniform(shape=(1, 1, in_filters, out_filters), dtype=tf.float32,\n",
    "                                              minval=-kernel_ds_xavier_range, maxval=+kernel_ds_xavier_range))\n",
    "    t_downsampled = tf.nn.conv2d(t, kernel_ds, strides=(1, stride, stride, 1), padding='SAME')\n",
    "    \n",
    "    kernel_c1_range = np.sqrt(6) / (np.sqrt(3*3*in_filters) + np.sqrt(out_filters))\n",
    "    kernel_c1 = tf.Variable(tf.random.uniform(shape=(3, 3, in_filters, out_filters), dtype=tf.float32,\n",
    "                                              minval=-kernel_c1_range, maxval=+kernel_c1_range))\n",
    "    t = tf.nn.conv2d(t, kernel_c1, strides=(1, stride, stride, 1), padding='SAME')\n",
    "    # FIXME - should also apply batch norm here to match Chen et al.\n",
    "    t = tf.nn.relu(t)\n",
    "    \n",
    "    kernel_c2_range = np.sqrt(6) / (np.sqrt(3*3*out_filters) + np.sqrt(out_filters))\n",
    "    kernel_c2 = tf.Variable(tf.random.uniform(shape=(3, 3, out_filters, out_filters), dtype=tf.float32,\n",
    "                                             minval=-kernel_c2_range, maxval=+kernel_c2_range))\n",
    "    t = tf.nn.conv2d(t, kernel_c2, strides=(1, 1, 1, 1), padding='SAME')\n",
    "    \n",
    "    # Skip connection\n",
    "    # N.B. No activation function. This is the sum of the output of two convolutions (one 3x3, the other 1x1).\n",
    "    return t + t_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that our residual block works gives the correct shape of outputs\n",
    "test_ph1 = tf.placeholder(shape=(None, 28, 28, 1), dtype=tf.float32)\n",
    "test_out1 = res_block(test_ph1, in_filters=1, out_filters=64, stride=2)\n",
    "print('Output shape should be [None, 14, 14, 64]: {}'.format(test_out1.shape))\n",
    "\n",
    "test_ph2 = tf.placeholder(shape=(None, 28, 28, 64), dtype=tf.float32)\n",
    "test_out2 = res_block(test_ph2, in_filters=64, out_filters=64, stride=1)\n",
    "print('Output shape should be [None, 28, 28, 64]: {}'.format(test_out2.shape))\n",
    "\n",
    "# Check that these will actually run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(test_out1, feed_dict={test_ph1: fake_input_batch})\n",
    "    sess.run(test_out2, feed_dict={test_ph2: np.tile(fake_input_batch, reps=(1, 1, 1, 64))}) # Tile to fake channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_net(input_tensor: tf.Tensor, n_filters: int) -> tf.Tensor:\n",
    "    # Convolution with 3x3 kernels\n",
    "    # N.B. Chen et al. do use biases in this conv.\n",
    "    kernel_range = np.sqrt(6) / (np.sqrt(3*3*1) + np.sqrt(n_filters))\n",
    "    kernel = tf.Variable(tf.random.uniform(shape=(3, 3, 1, n_filters), dtype=tf.float32,\n",
    "                                           minval=-kernel_range, maxval=+kernel_range))\n",
    "    biases = tf.Variable(tf.zeros(shape=(1, 1, 1, n_filters), dtype=tf.float32))\n",
    "    conv_output = tf.nn.conv2d(input_tensor, kernel, strides=(1, 1, 1, 1), padding='SAME') + biases\n",
    "    \n",
    "    return res_block(res_block(conv_output, in_filters=n_filters, out_filters=n_filters, stride=2), in_filters=n_filters, out_filters=n_filters, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ph = tf.placeholder(shape=(None, 28, 28, 1), dtype=tf.float32)\n",
    "test_out = downsample_net(test_ph, n_filters=64)\n",
    "\n",
    "print('Output shape should be [None, 7, 7, 64]: {}'.format(test_out.shape))\n",
    "\n",
    "# Check that this will actually run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(test_out, feed_dict={test_ph: fake_input_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWMA:\n",
    "    \n",
    "    def __init__(self, decay_lambda: float=0.1):\n",
    "        self._decay_lambda = decay_lambda\n",
    "        self._running_ewma = None\n",
    "    \n",
    "    def update(self, x: float):\n",
    "        \n",
    "        if self._running_ewma is None:\n",
    "            self._running_ewma = x\n",
    "        else:\n",
    "            self._running_ewma *= 1.0 - self._decay_lambda\n",
    "            self._running_ewma += self._decay_lambda * x\n",
    "    \n",
    "    def get(self):\n",
    "        return self._running_ewma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Define the network\n",
    "        self._input_img_batch = tf.placeholder(shape=(None, img_h, img_w, 1), dtype=tf.float32)\n",
    "        \n",
    "        downsampled_img = downsample_net(self._input_img_batch, n_filters=64)\n",
    "        dim_ds_img = (img_h//4)*(img_w//4)*64\n",
    "        \n",
    "        flattened_ds_img = tf.reshape(downsampled_img, shape=(-1, dim_ds_img))\n",
    "        flattened_act = tf.nn.relu(flattened_ds_img)\n",
    "        self._flattened_act = flattened_act\n",
    "        \n",
    "        W_range = np.sqrt(6) / (np.sqrt(dim_ds_img) + np.sqrt(10))\n",
    "        W_fc = tf.Variable(tf.random.uniform(shape=(dim_ds_img, 10), dtype=tf.float32,\n",
    "                                             minval=-W_range, maxval=+W_range), name='W_fc')\n",
    "        b_fc = tf.Variable(tf.zeros(shape=(10), dtype=tf.float32))\n",
    "        self._output_logits = tf.matmul(flattened_act, W_fc)\n",
    "\n",
    "       \n",
    "        # Define a loss for training\n",
    "        self._input_gt_oh = tf.placeholder(shape=(None, 10), dtype=tf.float32)\n",
    "        self.ce_loss = tf.losses.softmax_cross_entropy(logits=self._output_logits, onehot_labels=self._input_gt_oh)\n",
    "        \n",
    "        # Accuracy, for evaluation\n",
    "        self._output_class = tf.math.argmax(self._output_logits, axis=1)\n",
    "        self._gt_class = tf.math.argmax(self._input_gt_oh, axis=1)\n",
    "        self._correct_predictions = tf.math.equal(self._output_class, self._gt_class)\n",
    "        self._acc = tf.reduce_mean(tf.cast(self._correct_predictions, dtype=tf.float32), axis=0)\n",
    "        \n",
    "        self._train_step = tf.train.MomentumOptimizer(learning_rate=1e-2, momentum=0.9).minimize(self.ce_loss)\n",
    "    \n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self._running_ce = EWMA(decay_lambda=0.1)\n",
    "        self._ce_history = []\n",
    "        self._running_acc = EWMA(decay_lambda=0.1)\n",
    "        self._acc_history = []\n",
    "    \n",
    "    def train_batch(self, input_images, input_gt_oh):\n",
    "        \n",
    "        _, ce_this_batch, acc_this_batch = self._sess.run([self._train_step, self.ce_loss, self._acc],\n",
    "                                                          feed_dict={self._input_img_batch: input_images,\n",
    "                                                                     self._input_gt_oh: input_gt_oh})\n",
    "        \n",
    "        self._running_ce.update(ce_this_batch)\n",
    "        self._ce_history.append(self._running_ce.get())\n",
    "        self._running_acc.update(acc_this_batch)\n",
    "        self._acc_history.append(self._running_acc.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_and_label_set = ImageAndLabelSet(path.join(mnist_data_root, 'train-images-idx3-ubyte'),\n",
    "                                       path.join(mnist_data_root, 'train-labels-idx1-ubyte') )\n",
    "fc_net = FCNet()\n",
    "\n",
    "for b in range(1000):\n",
    "    image_batch, label_batch = image_and_label_set.getNextBatch(batchSize=32)\n",
    "    image_batch = np.reshape(image_batch, (-1, img_h, img_w, 1)) # Add a channel dimension\n",
    "    \n",
    "    fc_net.train_batch(image_batch, label_batch)\n",
    "    \n",
    "    if b % 100 == 0:\n",
    "        print('batch {}: CE: {}\\tacc: {}'.format(b, fc_net._running_ce.get(), fc_net._running_acc.get()))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fc_net._ce_history)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot([100.0*(1.0 - x) for x in fc_net._acc_history], 'r')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_ylabel('Error rate (%)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
